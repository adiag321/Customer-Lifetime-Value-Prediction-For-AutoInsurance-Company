name: CLV Modeling Pipeline - GitHub Actions

on:
  push:
    branches: [ main, develop ]
    paths:
      - '03_Modeling.py'
      - '01_Data_processing.py'
      - 'data/**'
      - 'requirements.txt'
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * 0'  # Weekly Sunday at 2 AM UTC
  workflow_dispatch:  # Manual trigger

env:
  PYTHON_VERSION: '3.9'

jobs:
  data-processing:
    name: Data Processing
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run data processing
      run: |
        echo "Starting data processing..."
        python 01_Data_processing.py
        echo "Data processing completed successfully!"
    
    - name: Verify processed data
      run: |
        python -c "
        import pandas as pd
        data = pd.read_csv('./data/Processed_AutoInsurance.csv')
        print(f'✓ Data shape: {data.shape}')
        print(f'✓ Columns: {data.columns.tolist()[:5]}...')
        print(f'✓ No missing values in CLV: {data[\"CLV\"].isnull().sum() == 0}')
        "

  model-training:
    name: Model Training & Evaluation
    runs-on: ubuntu-latest
    needs: data-processing
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run data processing
      run: python 01_Data_processing.py
    
    - name: Train models
      run: |
        echo "Starting model training pipeline..."
        python 03_Modeling.py --test-size 0.30 --verbose
        echo "Model training completed successfully!"
      timeout-minutes: 60
    
    - name: Verify results
      if: always()
      run: |
        python -c "
        import pandas as pd
        results = pd.read_csv('./results/model_results_summary.csv', index_col=0)
        print('\n=== Model Performance Summary ===')
        print(results.to_string())
        
        best_model = results['R2'].idxmax()
        best_r2 = results.loc[best_model, 'R2']
        best_rmse = results.loc[best_model, 'RMSE']
        
        print(f'\n✓ Best Model: {best_model}')
        print(f'✓ R² Score: {best_r2:.6f}')
        print(f'✓ RMSE: {best_rmse:.6f}')
        
        if best_r2 >= 0.85:
            print('✓ Performance meets quality threshold (R² >= 0.85)')
        else:
            print('⚠ Warning: R² below threshold. Consider model review.')
        "
    
    - name: Upload model results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: model-results
        path: |
          results/
        retention-days: 30
    
    - name: Generate summary
      if: always()
      run: |
        echo "## Model Training Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f "./results/model_results_summary.csv" ]; then
          echo "✓ Model training completed" >> $GITHUB_STEP_SUMMARY
          echo "✓ Results saved to: \`results/model_results_summary.csv\`" >> $GITHUB_STEP_SUMMARY
          echo "✓ Visualizations saved to: \`results/model_comparison_visualization.png\`" >> $GITHUB_STEP_SUMMARY
        else
          echo "✗ Model training failed - results not found" >> $GITHUB_STEP_SUMMARY
        fi

  quality-checks:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install linting tools
      run: |
        pip install pylint flake8 black isort
    
    - name: Run Black (code formatting check)
      run: black --check 03_Modeling.py 01_Data_processing.py || true
    
    - name: Run Flake8 (linting)
      run: flake8 03_Modeling.py 01_Data_processing.py --max-line-length=120 || true
    
    - name: Run isort (import sorting check)
      run: isort --check-only 03_Modeling.py 01_Data_processing.py || true

  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [model-training, quality-checks]
    if: always()
    
    steps:
    - name: Check workflow status
      run: |
        if [ "${{ needs.model-training.result }}" == "success" ]; then
          echo "✅ Pipeline completed successfully!"
          exit 0
        else
          echo "❌ Pipeline failed. Check logs for details."
          exit 1
        fi

  performance-tracking:
    name: Performance Tracking
    runs-on: ubuntu-latest
    needs: model-training
    if: success()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Create performance badge
      run: |
        python -c "
        import pandas as pd
        import json
        
        results = pd.read_csv('./results/model_results_summary.csv', index_col=0)
        best_r2 = results['R2'].max()
        
        badge_color = 'green' if best_r2 >= 0.85 else 'yellow' if best_r2 >= 0.75 else 'red'
        
        with open('model_metrics.json', 'w') as f:
            json.dump({
                'best_r2': round(best_r2, 4),
                'badge_color': badge_color,
                'timestamp': pd.Timestamp.now().isoformat()
            }, f)
        "
    
    - name: Upload performance metrics
      uses: actions/upload-artifact@v3
      with:
        name: performance-metrics
        path: model_metrics.json

